
    <html>
      <head>
  <title>AI-first, a modern anti-pattern.</title>
  <link rel="shortcut icon" type="image/x-icon" href="favicon.ico?">
    <meta charset="utf-8">
    <meta name="description" content="AI-first, a modern anti-pattern.">
        <meta name="viewport" content="width=device-width, initial-scale=1">
      </head>
      <body>
        <div id='content'>
    <h1 id="ai-first-a-modern-anti-pattern">AI-first, a modern anti-pattern.</h1>
<ul>
<li><a href="#the-goal">The goal</a><ul>
<li><a href="#targeted-audience">Targeted audience</a></li></ul></li>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#app-implementation">App implementation</a><ul>
<li><a href="#user-facing-product">User-facing product</a></li>
<li><a href="#ai-last">AI Last</a></li>
<li><a href="#mobile-app-framework">Mobile app framework</a></li>
<li><a href="#backend">Backend</a></li>
<li><a href="#labelling-ui">Labelling UI</a></li></ul></li>
<li><a href="#image-recognition">Image recognition</a><ul>
<li><a href="#existing-work">Existing work</a></li>
<li><a href="#iterations-with-opencv">Iterations with OpenCV</a></li>
<li><a href="#iterations-with-custom-trained-object-detection">Iterations with custom-trained object detection</a><ul>
<li><a href="#display-detection">Display detection</a></li>
<li><a href="#digit-detection">Digit detection</a></li></ul></li>
<li><a href="#results">Results</a></li></ul></li>
<li><a href="#conclusion">Conclusion</a><ul>
<li><a href="#ml-comes-last">ML comes last</a></li>
<li><a href="#the-state-of-object-detection-support">The state of Object Detection support</a></li>
<li><a href="#ml-principles-i-violated-and-got-burnt">ML principles I violated and got burnt</a></li>
<li><a href="#parting-words">Parting words</a></li></ul></li>
</ul>
<h2 id="the-goal">The goal</h2>
<p>This post shares my thoughts about:</p>
<ul>
<li>Rotation-invariant algorithm to detect a display on the image and read the weight value written on it</li>
</ul>
<p>In this article, I describe a robust solution for display orientation localization which is a preprocessor for the digit detection problem. On top of that, I would like to share my thoughts on the overall process of building an AI-powered product.</p>
<ul>
<li>The state of ML tools today available on Google Cloud</li>
</ul>
<p>The Google Cloud AutoML Object Detection and their whole AI Platform worked surprisingly well for me. Even bigger surprise is that the trained models can be exported so you can use a runtime on your own to run them! Once again, engineering time in real world applications will be spent on massaging the data.</p>
<ul>
<li>ML principles for real-world applications</li>
</ul>
<p>Unless you are writing a school project, or sharpening your ML skills, the deeper ML challenges in the real world projects come last. Engineering will be dominated by data selection / gathering / cleaning / reformatting and evaluation frameworks.</p>
<h3 id="targeted-audience">Targeted audience</h3>
<ul>
<li><p>Software Engineers, Data Scientists, Product Managers and other Rock and Roll people.</p></li>
<li><p>Less likely, if you are using <a href="https://snapscale.life">the photo weigh-in application</a>, here you can learn how the AI behind it works.</p></li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>I started drilling on SnapScale in mid-2018 with a goal of <strong>creating a weigh-in habit</strong> for a friend of mine. I converted their old school scale into a smart one <strong>using the phone's camera</strong>. The user scans the weight measurement with a phone and the app recognizes the value, logs it, and optionally exports to FitBit profile.</p>
<p>Feel free to try it out - no log-in or account creation is required - <a href="https://snapscale.life">SnapScale - build a weight-in habit</a>.</p>
<p>This report gives a timeline of an end-to-end real-world application heavily relying on ML tools.</p>
<h2 id="app-implementation">App implementation</h2>
<h3 id="user-facing-product">User-facing product</h3>
<p>The goal of SnapScale is to instill a weigh-in habit in humans. Taking a weight measurement has an amplified awareness effect if the number is logged, and if you can compare it with last week or last month. It's the trend that matters, not day-to-day oscillations.</p>
<p>The app lets you log the weight measurement by taking a photo of the scale's display while you're standing on the scale. User doesn't need to lean down - a photo taken from the normal hand height works. The AI model reads out the digits and logs the weight.</p>
<p>There are 3 screens in the app:</p>
<ul>
<li>submit a new weight-in measurement that launches a camera</li>
<li>view your previous measurements</li>
<li>settings screen that optionally sets up FitBit export and no-weigh-in reminders</li>
</ul>
<p align="center">
  <img src="image1.jpg" width="400" alt="SnapScale application home screen" border="1px">
</p>
<p>Tapping the weigh-in button launches the camera.</p>
<p>We intentionally reduced clutter that could waste the user's time in the app. There is no login required or account creating to start using the app. Optionally if a user wants to export their data to FitBit, the login with FitBit is required.</p>
<h3 id="ai-last">AI Last</h3>
<p>Engineering-wise, the central and most "sexy" part of the application is the component that reads the weight measurement from the image. I delayed tackling that part of the problem. Instead, I tried adopting the "AI Last" principle. That meant first doing the following:</p>
<ul>
<li>Built a mobile app for photo-based weight-logging, reminders, and export to FitBit.</li>
<li>Launched with a fake backend - each image would trigger a notification on my phone for me to read the numbers instead of the ML model</li>
<li>Implemented the UI for reading the numbers from images and labelling them</li>
<li>Implemented the evaluation framework for image recognition problem</li>
</ul>
<p>Only then, when everything was ready and it seemed that the product may work, I tackled the AI:</p>
<ul>
<li>picking the technology and implementing the image recognition solution</li>
</ul>
<p>This is what I mean by AI comes last.</p>
<p>By the time I started iterating on the model, I labeled ~450 images and had a good idea of all kinds of corner cases that might happen. I also had a dataset of images, defined metrics, debugging frontend and evaluation framework in place.</p>
<p>One risk of this approach is betting everything on the fact that the last step (AI) will work. It could've happened that OCR at the end would be too difficult to implement, and the project would "fail slowly".</p>
<h3 id="mobile-app-framework">Mobile app framework</h3>
<p>The application is built using <a href="https://expo.io">Expo</a> framework which is a wrapper around <a href="https://reactnative.dev/">React Native</a>.  It's suitable for fast prototyping which is exactly what I needed. Same codebase is executed on both platforms. If Expo had a book it would be called "App development for dummies".</p>
<h3 id="backend">Backend</h3>
<p>Backend is used to store weight logs and images that will be used for model training. The clients don't do any authentication, but there is a lightweight handshake step to guard from spamming.</p>
<p>Although login sounds like a good alternative, I suspect it would repel some users due to sensitivity of the images that are being sent. This way we keep user images stripped of any other kind of PII.</p>
<p>The database for metadata used is <a href="https://cloud.google.com/datastore">Google Cloud Datastore</a> and for the image store I used <a href="https://cloud.google.com/storage">Google Cloud Storage</a>. Both systems are free-of-charge for prototypes with low amount of requests. Also, no setup is needed.</p>
<p>The natural next step is to move the model execution to the client, which will remove a need for a backend, and simplify privacy policy.</p>
<h3 id="labelling-ui">Labelling UI</h3>
<p>When a new image is submitted to the system, and the model is not confident about the results (or when the model was not even there yet) an <a href="https://ifttt.com">IFTTT</a> notification would hit my phone with the link to the Labelling UI. The Labelling UI would show the image, let me translate and rotate it, and label the digits. The digits are labeled with rectangles that all share <code>y</code> coordinates, and all have the same dimensions. This is possible because prior to labelling the digits, the labeller rotates the image in such a way that the display is upright.</p>
<p align="center">
  <img src="image2.jpg" alt="SnapScale Labelling UI screenshot" border="1px">
</p>
<p>Note that I made an assumption that the digits are always monospace. Well :) Assumption turns out to be the mother of all fuckups.</p>
<p align="center">
  <img src="image3.jpg" alt="Non-monospace display digit" border="1px">
</p>
<p>I manually labeled ~450 images so far. Each image has 3-4 digits, each digit is two mouse clicks. The Labelling UI is implemented in HTML5, mainly using Canvas API in JavaScript. The image transformation logic heavily uses affine transformation matrices - <a href="https://developer.mozilla.org/en-US/docs/Web/API/DOMMatrix">DOMMatrix</a>.</p>
<p>The images without visible weight measurement are discarded. All other images are labeled and split into training/test/validation with 70/10/20 ratios.</p>
<p>Having only valid images in the dataset, I defined <code>accuracy</code> to be my main metric:</p>
<ul>
<li><strong>accuracy</strong>: percentage of images in which all digits are correctly identified</li>
</ul>
<p>If an image that shows <code>78.2</code> gets recognized as <code>78.3</code>, the accuracy is not satisfied.</p>
<h2 id="image-recognition">Image recognition</h2>
<p>I started working on Image recognition ~2 months in the project. At that time, I had an application that works, with 1-2 OCR requests per day all labeled manually. I was ready to implement the digit recognition submodule. I expected that I will find something that works out of the box, and that most of engineering will be just plugging it in and evaluating accuracy.</p>
<h3 id="existing-work">Existing work</h3>
<p>I found a bunch of existing related models that would in theory solve our problem. Reproducibility of those projects was low.</p>
<p>OpenCV approaches on GitHub:</p>
<ul>
<li><a href="https://github.com/jacnel/wetr">github.com/jacnel/wetr</a></li>
<li><a href="https://github.com/NatholBMX/ScaleDigitizer">github.com/NatholBMX/ScaleDigitizer</a></li>
<li><a href="https://github.com/charleswest/Weight">charleswest/Weight</a></li>
<li><a href="https://github.com/suyashkumar/seven-segment-ocr">suyashkumar/seven-segment-ocr</a></li>
<li><a href="https://github.com/kazmiekr/GasPumpOCR.git">kazmiekr/GasPumpOCR.git</a></li>
<li><a href="https://github.com/bikz05/digit-recognition">bikz05/digit-recognition</a></li>
<li><a href="https://github.com/g-sari/pyautodigits">g-sari/pyautodigits</a></li>
</ul>
<p>OpenCV approaches articles:</p>
<ul>
<li><a href="http://www.dccia.ua.es/~sco/ComputerVision/embedded.pdf">Reading LCD/LED Displays with a Camera Cell Phone</a></li>
<li><a href="https://hackernoon.com/building-a-gas-pump-scanner-with-opencv-python-ios-116fe6c9ae8b">Building a Gas Pump Scanner with OpenCV/Python/iOS</a>)</li>
<li><a href="https://towardsdatascience.com/build-a-multi-digit-detector-with-keras-and-opencv-b97e3cd3b37">Build a Multi Digit Detector with Keras and OpenCV</a></li>
<li><a href="https://towardsdatascience.com/scanned-digits-recognition-using-k-nearest-neighbor-k-nn-d1a1528f0dea">Scanned Numbers Recognition using k-Nearest Neighbor (k-NN)</a></li>
<li><a href="https://www.kurokesu.com/main/2017/02/20/dumb-thermometer-gets-digital-output/">Dumb thermometer gets digital output</a></li>
<li><a href="https://www.pyimagesearch.com/2017/02/13/recognizing-digits-with-opencv-and-python/">Recognizing digits with OpenCV and Python</a></li>
<li><a href="https://www.sciencedirect.com/science/article/pii/S235248471930174X">Text detection and recognition in raw image dataset of seven segment digital energy meter display</a></li>
</ul>
<p>Street View House Number Dataset related solutions:</p>
<ul>
<li><a href="https://medium.com/project-agi/getting-started-with-street-view-house-numbers-svhn-dataset-a96e76962504">Getting Started with ‘Street View House Numbers’ (SVHN) Dataset</a></li>
<li><a href="https://arxiv.org/pdf/1312.6082.pdf">Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks</a></li>
<li><a href="http://ufldl.stanford.edu/housenumbers/">The Street View House Numbers (SVHN) Dataset</a></li>
<li><a href="https://github.com/potterhsu/SVHNClassifier">github.com/potterhsu/SVHNClassifier</a></li>
<li><a href="https://github.com/znat/udacity-digit-recognition-program-svhn/blob/master/project-report/project-report.md">A SVHN-related project report</a></li>
</ul>
<p>Open Source Tensorflow Object Detection pipeline:</p>
<ul>
<li><a href="https://github.com/tensorflow/models/tree/master/research/object_detection">github.com/tensorflow/models research/object_detection</a></li>
</ul>
<p>Documentation about Google Cloud object detection solutions as service:</p>
<ul>
<li><a href="https://cloud.google.com/vision">Google Cloud Vision API</a></li>
<li><a href="https://cloud.google.com/vision/automl/object-detection/docs">Google Cloud AutoML Vision Object Detection</a></li>
<li><a href="https://cloud.google.com/ai-platform/training/docs/algorithms/object-detection-start">Google Cloud AI Platform built-in Object Detection</a></li>
</ul>
<p>My time in investigating and trying out existing solutions was roughly spent like this:</p>
<ul>
<li><code>30%</code> Classical computer vision techniques (OpenCV and similar)</li>
<li><code>30%</code> SVHN dataset</li>
<li><code>2%</code> Google Cloud Vision API</li>
<li><code>38%</code> Tensorflow custom solutions and transfer learning</li>
</ul>
<p>I implemented 5-6 different pipelines that will take SnapScale images and labels and convert them to a format that the solutions above take :) That's a lot of hours spent debugging bugs in geometry and image processing code.</p>
<p>I didn't expect to get in a situation where I'll be (re)training new models. Especially because I had only 450 images available.</p>
<h3 id="iterations-with-opencv">Iterations with OpenCV</h3>
<p>I spent 2-3 weeks trying to tune OpenCV algorithms to preprocess the images into binary images where the digits are distinguished from background. This is the usual preprocessing step used by related work from the first two sections above. Assuming that this step is successful, the following phases would run connected components to isolate each digit and then run shape classification to recognize the final number.</p>
<p>Here are some examples where OpenCV <code>adaptiveThreshold</code> was able to do the job correctly after painful tuning process:</p>
<p align="center">
  <img src="good.jpg" alt="Good binarization examples" border="1px">
</p>
<p>Unfortunately a good amount of images looked more challenging for binarization:</p>
<p align="center">
  <img src="bin.jpg" alt="Bad binarization examples" border="1px">
</p>
<p>My confidence further deflated when I figured that many of the images in the set are not sharp .. that screws up both binarization and later contour detection:</p>
<p align="center">
  <img src="bad.jpg" alt="Contour detection problems" border="1px">
</p>
<p>As negative examples were not a minority of my training set, I ultimately gave up on OpenCV and started looking into ML models. My next step was to use SVHN (Street View House Numbers) dataset, but I'll skip the details there.. Long story short is that SVHN numbers are very different from the display numbers. Finally, I decided to train a custom model using the training set I collected.</p>
<p>Spoiler alert - all images from the above examples were correctly classified with Tensorflow Object Detection explained below.</p>
<h3 id="iterations-with-custom-trained-object-detection">Iterations with custom-trained object detection</h3>
<p>From their documentation, <a href="https://cloud.google.com/vision/automl/object-detection/docs">Google Cloud AutoML Vision Object Detection</a> looks too good to be true. The Web UI enables you to upload images, train the model and see evaluation results. The trainer uses transfer learning which enables it to train something useful even with a few hundred images only. No setup, no config.</p>
<p>Naive as I am, I started by training a model that solves everything at once: read digits from raw user image. The input is the raw user image, and the output are labeled digit rectangles.</p>
<p>This hit two problems:</p>
<ul>
<li>the 7-segment digits sometimes change meaning with rotation (think 2 and 5)</li>
<li>the digit was sometimes too small on the original image for the model to pick up</li>
</ul>
<p>The rotation problem is amplified by the fact that the mobile phone doesn't know how to orientate the image of something that is on the floor.</p>
<p>I decided to split the problem to two phases:</p>
<ul>
<li>detecting the display in the big image,</li>
<li>detecting the digits in the display.</li>
</ul>
<p>Each phase can be evaluated on its own, which is useful in loss analysis.</p>
<h4 id="display-detection">Display detection</h4>
<p>My first attempt was predicting a single rectangle of a single class <code>display</code> on the input image. The training set was consisting of upright rotated images labeled with a single rectangle around the display. The AutoML Object Detection learned this with very good precision. At prediction time I would rotate the image 4 times and find the maximum confidence score for the display detection. I assumed that the maximum confidence would be achieved when the display is in upright rotation, just as it was in the training set.</p>
<p>I was wrong. I violated the property that training input distribution needs be aligned with the runtime input distribution. The model training saw only correctly rotated images, so the confidences that I was getting for the incorrectly rotated images were rubbish - sometimes 0, sometimes 1.</p>
<p align="center">
  <img src="image4.jpg" alt="Display detection" border="1px">
</p>
<p>The green detection box represents the input that was expected by the model, and the scores there are precise. The red detection boxes have scores that have no guarantees.</p>
<p>To fix this, I made the problem harder for the model. I expanded each image in the input to <code>K=4</code> new images with different rotations. Rotations are done in <code>360 / K = 90</code> increments, starting from the upright image. The display label on the upright image became <code>display_rotated_0_degrees</code>. On the next image it became <code>display_rotated_90_degrees</code>, then <code>display_rotated_180_degrees</code> and <code>display_rotated_270_degrees</code>. So the number of input images and output classes grew <code>K</code> times.</p>
<p align="center">
  <img src="image5.jpg" alt="Display detection with multiple rotations" border="1px">
</p>
<p>During the prediction I would show the input image to the model. The output is the display location and how much it was rotated from the upward position.</p>
<p>This worked much better, but it still had some precision losses. The final slam dunk for display+rotation detection was the voting system during prediction. Because the training phase received <code>K</code> rotations of each image, now it became OK to present the model with <code>K</code> rotations of the input image during prediction time. Each of the <code>K</code> rotations would give some answer to where the display is and how it's rotated.</p>
<p>If the model was perfect, all evaluations would show the same display location, and rotation hints would yield different values (<code>0</code>, <code>90</code>, <code>180</code>, <code>270</code>) for each rotation.</p>
<p>The final answer is given by subtracting the model's rotation hint from the image rotation given to the model. Implementing a voting scheme here gave surprisingly good results.</p>
<p>Note that the hyper-parameter <code>K</code> doesn't need to be <code>4</code>. If the model is perfect, the maximum angle distance of the hinted rotation and the correct rotation is <code>(360 / K) / 2</code>. If <code>K</code> is bigger, the display detection problem is more difficult. On the other hand, the outputs are closer to being upright rotated, which makes the digit detection problem less difficult.</p>
<h4 id="digit-detection">Digit detection</h4>
<p>Digit detection inputs an upright display image and outputs rectangles labeled as digits: <code>digit_{0-9}</code>.</p>
<p align="center">
  <img src="image6.jpg" alt="Digit detection problem" border="1px">
</p>
<p>I expected this to be simple for the AutoML Object Detection. Wrong again! Nothing is simple. I got many precision losses which I couldn't easily explain.</p>
<p align="center">
  <img src="image7.jpg" alt="2-5 confusion in digit detection" border="1px">
</p>
<p>Looking through the open sourced Object Detection Tensorflow implementation on GitHub, I found a parameter <a href="https://github.com/tensorflow/models/blob/master/official/vision/detection/dataloader/shapemask_parser.py#L131"><code>aug_rand_flip</code></a>. Based on the docs, the parameter expands input training images with their horizontal flips. This parameter can't be configured in AutoML Object Detection, but if they set it to true it would surely screw up 7-segment digit object detection.</p>
<p align="center">
  <img src="image8.jpg" alt="Flip horizontaly loses information about the 7-segment digits" border="1px">
</p>
<p>Google Cloud support answered my question on <a href="https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/cloud-vision-discuss/6mrbUdWcVys/Tv4nXRy8BAAJ">cloud-vision-discuss thread</a>, which helped me understand that AI Platform Object Detection and AutoML Object Detection are quite different.</p>
<p>I submitted a training job to AI Platform (after building yet another pipeline to transform my data), and got excellent results. This was another surprise, because my training set was less than 400 images, and the AI Platform Object Detection trains from scratch - it doesn't do transfer learning.</p>
<h3 id="results">Results</h3>
<p>In the last iteration I used 468 images:</p>
<ul>
<li>317 images in training set</li>
<li>50 images in test set</li>
<li>101 images in validation set</li>
</ul>
<p>I never saw debug visualizations or debug logs of the images in the validation set - the evaluation pipeline would never produce them.</p>
<p>The final accuracies (the weight value correctly recognized):</p>
<ul>
<li>98.74% in training set</li>
<li>94.00% in test set</li>
<li>93.07% in validation set</li>
</ul>
<p>The detection latency is ~15 seconds, mostly because I optimized serving to be price efficient (free).</p>
<h2 id="conclusion">Conclusion</h2>
<h3 id="ml-comes-last">ML comes last</h3>
<p>Don't start your idea pitches with "We will use ML to do XYZ", just say "We will do XYZ".</p>
<p>Even if ML will be a backbone tool in your solution the engineering is still dominated by all other (less sexy) tasks:</p>
<ul>
<li>building infrastructure for collecting the data</li>
<li>implementing evaluation frameworks</li>
<li>implementing debugging tools</li>
<li>data transformation pipelines</li>
</ul>
<p>My opinion is that great engineering practices can be shown in any engineering challenge. The sexiness or non-sexiness of problems is irrelevant (unless you're recruiting new grads).</p>
<h3 id="the-state-of-object-detection-support">The state of Object Detection support</h3>
<p>I am surprised by the simplicity and efficiency of Google Cloud AutoML Object Detection. When it comes to pricing, each account gets $300 free quota which is enough to train 5-10 model iterations. If you are lucky to get the result you need in that quota, everything else can be free - they give the ability to download the trained model as a Docker container, TF.js bundle or TF Lite bundle.</p>
<p>Using this free quota I managed to train a successful weight scale measurement OCR system.</p>
<h3 id="ml-principles-i-violated-and-got-burnt">ML principles I violated and got burnt</h3>
<p><strong>The distribution of data presented to the model in production must align with the validation set distribution.</strong></p>
<p>I thought that SVHN trained models would do a good job in recognizing scale weight measurement numbers. The transfer learning from the SVHN model to my problem space also didn't end up working.</p>
<p>In display orientation detection, I thought that the model trained on upright display images would give a low score to input images that are not upright. The score was not low - it was rubbish, sometimes high, sometimes low.</p>
<p><strong>Be evaluation driven.</strong></p>
<p>Early in the project when I was evaluating the related work, I would try to run the existing model and then feed it with an image or two to see how it works. Only when my custom quality iterations started, I built a pipeline that runs the model on all train/test/val images and reports the metrics.</p>
<p>It would be better if I defined a container interface early on that listens to the image request on HTTP and gives a standardised JSON output. Then I would have a Docker image for every related work I managed to successfully run, persisted together with evaluation results. Having not done so, I only have weak (empirical) evidence that the related work models didn't work on my problem space.</p>
<p>When it comes to running things from other research-y works, the dependencies pose a big challenge. Docker solves this problem completely, and I am looking forward to seeing Docker adoption in academia.</p>
<h3 id="parting-words">Parting words</h3>
<p>On Monday, 2019-10-28 I was in a room in downtown Manhattan packed with excited engineers and PMs who were ready to take over the world. This was AWS Data Lake Day in AWS Loft. I clearly remember a specific sentence - the speaker said confidently:</p>
<blockquote>
  <p>You've gotta use ML! If you don't, chances are that your competitors will.</p>
</blockquote>

<p>I didn’t like this. On the other hand, in the audience I saw ripples of head-nodding folks getting high on agreement with the speaker.</p>

<p>I am all for using AI when it solves the user’s problem, so my engineering principle stays — “always user first”. ML is a tool, not a goal.</p>

        </div>
        <style type='text/css'>body {
  font: 400 16px/1.5 "Helvetica Neue", Helvetica, Arial, sans-serif;
  color: #111;
  background-color: #fdfdfd;
  -webkit-text-size-adjust: 100%;
  -webkit-font-feature-settings: "kern" 1;
  -moz-font-feature-settings: "kern" 1;
  -o-font-feature-settings: "kern" 1;
  font-feature-settings: "kern" 1;
  font-kerning: normal;
  padding: 30px;
}

@media only screen and (max-width: 600px) {
  body {
    padding: 5px;
  }

  body > #content {
    padding: 0px 20px 20px 20px !important;
  }
}

body > #content {
  margin: 0px;
  max-width: 900px;
  border: 1px solid #e1e4e8;
  padding: 10px 40px;
  padding-bottom: 20px;
  border-radius: 2px;
  margin-left: auto;
  margin-right: auto;
}

hr {
  color: #bbb;
  background-color: #bbb;
  height: 1px;
  flex: 0 1 auto;
  margin: 1em 0;
  padding: 0;
  border: none;
}

/**
 * Links
 */
a {
  color: #0366d6;
  text-decoration: none; }
  a:visited {
    color: #0366d6; }
  a:hover {
    color: #0366d6;
    text-decoration: underline; }

pre {
  background-color: #f6f8fa;
  border-radius: 3px;
  font-size: 85%;
  line-height: 1.45;
  overflow: auto;
  padding: 16px;
}

/**
  * Code blocks
  */

code {
  background-color: rgba(27,31,35,.05);
  border-radius: 3px;
  font-size: 85%;
  margin: 0;
  word-wrap: break-word;
  padding: .2em .4em;
  font-family: SFMono-Regular,Consolas,Liberation Mono,Menlo,Courier,monospace;
}

pre > code {
  background-color: transparent;
  border: 0;
  display: inline;
  line-height: inherit;
  margin: 0;
  overflow: visible;
  padding: 0;
  word-wrap: normal;
  font-size: 100%;
}


/**
 * Blockquotes
 */
blockquote {
  margin-left: 30px;
  margin-top: 0px;
  margin-bottom: 16px;
  border-left-width: 3px;
  padding: 0 1em;
  color: #828282;
  border-left: 4px solid #e8e8e8;
  padding-left: 15px;
  font-size: 18px;
  letter-spacing: -1px;
  font-style: italic;
}
blockquote * {
  font-style: normal !important;
  letter-spacing: 0;
  color: #6a737d !important;
}

/**
 * Tables
 */
table {
  border-spacing: 2px;
  display: block;
  font-size: 14px;
  overflow: auto;
  width: 100%;
  margin-bottom: 16px;
  border-spacing: 0;
  border-collapse: collapse;
}

td {
  padding: 6px 13px;
  border: 1px solid #dfe2e5;
}

th {
  font-weight: 600;
  padding: 6px 13px;
  border: 1px solid #dfe2e5;
}

tr {
  background-color: #fff;
  border-top: 1px solid #c6cbd1;
}

table tr:nth-child(2n) {
  background-color: #f6f8fa;
}

/**
 * Others
 */

img {
  max-width: 100%;
}

p {
  line-height: 24px;
  font-weight: 400;
  font-size: 16px;
  color: #24292e; }

ul {
  margin-top: 0; }

li {
  color: #24292e;
  font-size: 16px;
  font-weight: 400;
  line-height: 1.5; }

li + li {
  margin-top: 0.25em; }

* {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
  color: #24292e; }

a:visited {
  color: #0366d6; }

h1, h2, h3 {
  border-bottom: 1px solid #eaecef;
  color: #111;
  /* Darker */ }</style>
      </body>
    </html>
