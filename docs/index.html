
    <html>
      <head>
  <title>SnapScale: technical report</title>
  <link rel="shortcut icon" type="image/x-icon" href="favicon.ico?">
    <meta charset="utf-8">
    <meta name="description" content="SnapScale engineering report">
        <meta name="viewport" content="width=device-width, initial-scale=1">
      </head>
      <body>
        <div id='content'>
    <h1 id="snapscale-technical-report">SnapScale: technical report</h1>
<ul>
<li><a href="#introduction">Introduction</a><ul>
<li><a href="#the-goal-of-this-report">The goal of this report</a></li>
<li><a href="#targeted-audience">Targeted audience</a></li></ul></li>
<li><a href="#case-study-a-weigh-in-habit-app-snapscale">Case study: a weigh-in habit app, SnapScale</a><ul>
<li><a href="#why">Why</a></li>
<li><a href="#how">How</a></li></ul></li>
<li><a href="#app-implementation">App implementation</a><ul>
<li><a href="#ai-last">AI Last</a></li>
<li><a href="#the-app">The app</a></li>
<li><a href="#backend">Backend</a></li>
<li><a href="#labelling-ui">Labelling UI</a></li></ul></li>
<li><a href="#image-recognition">Image recognition</a><ul>
<li><a href="#existing-work">Existing work</a></li>
<li><a href="#iterations-with-custom-trained-object-detection">Iterations with custom-trained object detection</a><ul>
<li><a href="#display-detection">Display detection</a></li>
<li><a href="#digit-detection">Digit detection</a></li></ul></li>
<li><a href="#results">Results</a></li></ul></li>
<li><a href="#conclusion">Conclusion</a><ul>
<li><a href="#the-state-of-object-detection-support">The state of Object Detection support</a></li>
<li><a href="#ml-principles-i-violated-and-got-burnt">ML principles I violated and got burnt</a></li>
<li><a href="#parting-words">Parting words</a></li></ul></li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>I started drilling on SnapScale in mid-2018 with a goal of <strong>creating a weigh-in habit</strong> for a friend of mine. The idea is to convert an old school scale into a smart one <strong>using phone's camera</strong>. User scans the weight measurement with a phone camera and the app recognizes the value, logs it, and optionally exports to user's FitBit profile.</p>
<p>Feel free to try it out - no log-in or account creation is required - <a href="https://snapscale.life">SnapScale - build a weight-in habit</a>.</p>
<p>This report gives a timeline of an end to end project heavly relying on ML.</p>
<h3 id="the-goal-of-this-report">The goal of this report</h3>
<p>I am writing this report to share my thoughts about:</p>
<ul>
<li>ML principles for real-world applications</li>
<li>The state of ML tools today available on Google Cloud</li>
<li>Rotation-invariant algorithm to detect a display on the image and read the weight value written on it</li>
</ul>
<h3 id="targeted-audience">Targeted audience</h3>
<p>Software Engineers, Data Scientists, Product Managers and other Rock and Roll people.</p>
<p>Less likely, if you are using <a href="https://snapscale.life">the photo weigh-in application</a>, here you can learn how the AI behind it works.</p>
<h2 id="case-study-a-weigh-in-habit-app-snapscale">Case study: a weigh-in habit app, SnapScale</h2>
<h3 id="why">Why</h3>
<p>The goal of SnapScale is to instill a weigh-in habit in humans.</p>
<p>Taking a weight measurement has amplified awareness effect if the number is logged, and if you can compare it with last week, or last month. It's the trend that matters, not day-to-day oscillations.</p>
<h3 id="how">How</h3>
<p>Smart scales like FitBit Aria are a great way achieve this. With <a href="https://snapscale.life">SnapScale</a> we are giving the same product to people that don't own a smart scale.</p>
<p>SnapScale lets you log the weight measurement by taking a photo of scale's display while you're standing on the scale. User doesn't need to lean down - a photo taken from the normal hand height works. The AI model reads out the digits and logs the weight.</p>
<p>SnapScale is a simple application that has 3 screens:</p>
<ul>
<li>submit a new weight-in measurement that launches a camera</li>
<li>view your previous measurements</li>
<li>settings screen that optionally sets up FitBit export and no-weigh-in reminders</li>
</ul>
<p align="center">
  <img src="image1.jpg" width="400" alt="SnapScale application home screen" border="1px">
</p>
<p>Tapping the weigh-in button launches the camera.</p>
<p>We intentionally reduced clutter that could waste user's time in the app. There is no login required or account creating to start using the app. Optionally if user wants to export their data to FitBit, the login with FitBit is required.</p>
<h2 id="app-implementation">App implementation</h2>
<h3 id="ai-last">AI Last</h3>
<p>Engineering-wise, the central and most "sexy" part of the application is the component that reads the weight measurement from the image. But, in this project I tried adopting the "AI Last" principle. Before diving into OCR implementation details I first:</p>
<ul>
<li>Built a mobile app for photo-based weight-logging, reminders, and export to FitBit.</li>
<li>Launched th with a fake backend - each image would trigger a notification on my phone for me to read the numbers instead of the ML model</li>
<li>Implemented the UI for reading the numbers from images and labelling them</li>
<li>Implemented the evaluation framework for image recognition problem</li>
</ul>
<p>Only then, when everything was ready and it seemed that the product may work, I tackled the AI:</p>
<ul>
<li>picking the technology and implementing the image recognition solution</li>
</ul>
<p>This is what I mean by AI comes last.</p>
<p>By the time I started iterating on the model, I labeled ~450 images and had a good idea of all kinds of corner cases that might happen. I also had a dataset of images, defined metrics, debugging frontend and evaluation framework in place.</p>
<p>One risk of this approach is betting everything on the fact that the last step (AI) will work. It could've happened that OCR at the end would be too difficult to implement, and the project would "fail slow".</p>
<h3 id="the-app">The app</h3>
<p>The application is built using <a href="https://expo.io">Expo</a> framework which is a wrapper around <a href="https://reactnative.dev/">React Native</a>.  It's suitable for fast prototyping which is exactly what I needed. Same codebase is executed on both platforms. If Expo would have a book it would be called "App development for dummies".</p>
<h3 id="backend">Backend</h3>
<p>Backend is used to store weight logs and images. The clients don't do any authentication, but there is a lightweight handshake step.</p>
<p>On app install, user agrees to terms of use by signing with a captcha. The successful captcha challenge gives a permanent session token to the application that is later required to communicate with the backend. I find this trick useful when designing an application + backend storage without requiring a login.</p>
<p>Although login sounds like a good alternative, I suspect it would repel some users due to sensitivity of the images that are being sent. This way we keep user images stripped of any other kind of PII.</p>
<p>Backend is deployed as a <a href="https://nodejs.org">NodeJS</a> application on <a href="https://cloud.google.com/appengine">Google App Engine</a>. To get a free-of-charge deployment for prototypes with low amount of requests, the trick is to use <code>standard</code> environment with <code>F1</code> instance clasee. The <code>app.yaml</code> config looks like this:</p>
<pre><code>runtime: nodejs12
env: standard
instance_class: F1
</code></pre>
<p>The database for metadata used is <a href="https://cloud.google.com/datastore">Google Cloud Datastore</a> and for image store I used <a href="https://cloud.google.com/storage">Google Cloud Storage</a>. Both systems are free-of-charge for prototypes with low amount of requests. Also, no setup is needed.</p>
<p>The natural next step is to move the model execution to the client, which will remove a need for a backend, and simplify privacy policy.</p>
<h3 id="labelling-ui">Labelling UI</h3>
<p>When a new image is submitted to the system, and the model is not cofident about the results (or when the model was not even there yet) an IFTTT notification would hit my phone with the link to the Labelling UI. The Labelling UI would show the image, let me translate and rotate it, and label the digits. The digits are labeled with rectangles that all share <code>y</code> coordinates, and all have the same dimensions. This is possible because prior to labelling the digits, the labeler rotates the image in such way that the display is upright.</p>
<p align="center">
  <img src="image2.jpg" alt="SnapScale Labelling UI screenshot" border="1px">
</p>
<p>Note that I made an assumption that the digits are always monospace. Well :) Assumption turns out to be the mother of all fuckups.</p>
<p align="center">
  <img src="image3.jpg" alt="Non-monospace display digit" border="1px">
</p>
<p>I manually labeled ~450 images so far. Each image has 3-4 digits, each digit is two mouse clicks. The Labelling UI is implemented in HTML5, mainly using Canvas API in JavaScript. The image transformation logic heavily uses affine transformation matrices - <a href="https://developer.mozilla.org/en-US/docs/Web/API/DOMMatrix">DOMMatrix</a>.</p>
<p>The images without visible weight measurement are discarded. All other images are labeled and split into training/test/validation with 70/10/20 ratios.</p>
<p>Having only valid images in the dataset, I defined <code>accuracy</code> to be my main metric:</p>
<ul>
<li><strong>accuracy</strong>: percentage of images in which all digits are correctly identified</li>
</ul>
<p>If an image that shows <code>78.2</code> gets recognized as <code>78.3</code>, the accuracy is not satifsfied.</p>
<h2 id="image-recognition">Image recognition</h2>
<p>In this moment, ~2 months in the project I had an application that works, with 1-2 requests per day all labeled manually. I was ready to implement the digit recognition submodule. I expected that I will find something that works out of the box, and that most of engineering will be just plugging it in and evaluating accuracy.</p>
<h3 id="existing-work">Existing work</h3>
<p>I found a bunch of existing related models that would in theory solve our problem, but reproducibility was very low.</p>
<p>Classical computer vision techniques on GitHub:</p>
<ul>
<li><a href="https://github.com/jacnel/wetr">github.com/jacnel/wetr</a></li>
<li><a href="https://github.com/NatholBMX/ScaleDigitizer">github.com/NatholBMX/ScaleDigitizer</a></li>
<li><a href="https://github.com/charleswest/Weight">charleswest/Weight</a></li>
<li><a href="https://github.com/suyashkumar/seven-segment-ocr">suyashkumar/seven-segment-ocr</a></li>
<li><a href="https://github.com/kazmiekr/GasPumpOCR.git">kazmiekr/GasPumpOCR.git</a></li>
<li><a href="https://github.com/bikz05/digit-recognition">bikz05/digit-recognition</a></li>
<li><a href="https://github.com/g-sari/pyautodigits">g-sari/pyautodigits</a></li>
</ul>
<p>Classical computer vision techniques articles:</p>
<ul>
<li><a href="http://www.dccia.ua.es/~sco/ComputerVision/embedded.pdf">Reading LCD/LED Displays with a Camera Cell Phone</a></li>
<li><a href="https://hackernoon.com/building-a-gas-pump-scanner-with-opencv-python-ios-116fe6c9ae8b">Building a Gas Pump Scanner with OpenCV/Python/iOS</a>)</li>
<li><a href="https://towardsdatascience.com/build-a-multi-digit-detector-with-keras-and-opencv-b97e3cd3b37">Build a Multi Digit Detector with Keras and OpenCV</a></li>
<li><a href="https://towardsdatascience.com/scanned-digits-recognition-using-k-nearest-neighbor-k-nn-d1a1528f0dea">Scanned Numbers Recognition using k-Nearest Neighbor (k-NN)</a></li>
<li><a href="https://www.kurokesu.com/main/2017/02/20/dumb-thermometer-gets-digital-output/">Dumb thermometer gets digital output</a></li>
<li><a href="https://www.pyimagesearch.com/2017/02/13/recognizing-digits-with-opencv-and-python/">Recognizing digits with OpenCV and Python</a></li>
<li><a href="https://www.sciencedirect.com/science/article/pii/S235248471930174X">Text detection and recognition in raw image dataset of seven segment digital energy meter display</a></li>
</ul>
<p>Street View House Number Dataset related solutions:</p>
<ul>
<li><a href="https://medium.com/project-agi/getting-started-with-street-view-house-numbers-svhn-dataset-a96e76962504">Getting Started with ‘Street View House Numbers’ (SVHN) Dataset</a></li>
<li><a href="https://arxiv.org/pdf/1312.6082.pdf">Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks</a></li>
<li><a href="http://ufldl.stanford.edu/housenumbers/">The Street View House Numbers (SVHN) Dataset</a></li>
<li><a href="https://github.com/potterhsu/SVHNClassifier">github.com/potterhsu/SVHNClassifier</a></li>
<li><a href="https://github.com/znat/udacity-digit-recognition-program-svhn/blob/master/project-report/project-report.md">A SVHN-related project report</a></li>
</ul>
<p>Open Source Tensorflow Object Detection pipeline:</p>
<ul>
<li><a href="https://github.com/tensorflow/models/tree/master/research/object_detection">github.com/tensorflow/models research/object_detection</a></li>
</ul>
<p>Documentation about Google Cloud object detection solutions as service:</p>
<ul>
<li><a href="https://cloud.google.com/vision">Google Cloud Vision API</a></li>
<li><a href="https://cloud.google.com/vision/automl/object-detection/docs">Google Cloud AutoML Vision Object Detection</a></li>
<li><a href="https://cloud.google.com/ai-platform/training/docs/algorithms/object-detection-start">Google Cloud AI Platform built-in Object Detection</a></li>
</ul>
<p>My time in investigating and trying out existing solutions was roughly spent like this:</p>
<ul>
<li><code>30%</code> Classical computer vision techniques (OpenCV and similar)</li>
<li><code>30%</code> SVHN dataset</li>
<li><code>2%</code> Google Cloud Vision API</li>
<li><code>38%</code> Tensorflow custom solutions and transfer learning</li>
</ul>
<p>I implemented 5-6 different pipelines that will take my images and labels and convert them to a format that the solutions above take :) That's a lot of hours spent debugging bugs in geometry and image processing code.</p>
<p>I didn't expect to get in a situation where I'll be (re)training new models. Especially because I had only 400 images.</p>
<h3 id="iterations-with-custom-trained-object-detection">Iterations with custom-trained object detection</h3>
<p>From their documentation, Google Cloud AutoML Vision Object Detection looks too good to be true. The Web UI enables you to upload images, train the model and see evaluation results. The trainer uses transfer learning which enables it to train something useful even with a few hundred images only. No setup, no config.</p>
<p>Naive as I am, I started by training a model that solves everything at once. The input is the raw user image, and the output are labeled digit rectangles.</p>
<p>This hit two problems:</p>
<p>1) the 7-segment digits sometimes change meaning with rotation (think 2 and 5)<br />
1) the digit was sometimes too small on the original image for the model to pick up</p>
<p>The rotation problem is amplified by the fact that the mobile phone doesn't know how to orientat the image of something that is on the floor.</p>
<p>I decided to split the problem to two phases:</p>
<p>1) detecting the display in the big image,<br />
2) detecting the digits in the display.</p>
<p>Each phase can be evaluated on its own, which is useful in loss analysis.</p>
<h4 id="display-detection">Display detection</h4>
<p>My first attempt was predicting a single rectangle of a single class <code>display</code> on the input image. The training set was consisting of upright rotated images labeled with a single rectangle around the display. The AutoML Object Detection learned this with very good precision. At prediction time I would rotate the image 4 times and find the maximum confidence score for the display detection. I assumed that the maximum confidence would be achieved when the display is in upright rotation, just as it was in the training set.</p>
<p align="center">
  <img src="image4.jpg" alt="SnapScale application home screen" border="1px">
</p>
<p>The green detection box represents the input that was expected by the model, and the score is high. The red detection boxes have scores that have no guarantees.</p>
<p>Lol, no. I violated the property that training input distribution needs be aligned with the runtime input distribution. The model training saw only correctly rotated images, so the confidences that I was getting for the incorretly rotated images were rubbish - sometimes 0, sometimes 1.</p>
<p>To fix this, I made the problem harder for the model. I expanded each image in the input to <code>K=4</code> new images with different rotations. Rotations are done in <code>360 / K = 90</code> increments, starting from the upright image. The display label on the upright image became <code>display_rotated_0_degrees</code>. On the next image it became <code>display_rotated_90_degrees</code>, then <code>display_rotated_180_degrees</code> and <code>display_rotated_270_degrees</code>. So the number of input images and output classes grew <code>K</code> times.</p>
<p>(image5, K-rotation input)</p>
<p>During the prediction I would show the input image to the model. The output is the display location and how much it was rotated from the upward position.</p>
<p>(image6, K-rotation output)</p>
<p>This worked much better, but it still had some precission losses. The final slam dunk for display+rotation detection was the voting system during prediction. Because the training phase recieved <code>K</code> rotations of each image, now it became OK to present the model with <code>K</code> rotations of the input image during prediction time. Each of the <code>K</code> rotations would give some answer to where is the display and how it's rotated.</p>
<p>If the model was perfect, all evaluations would show the same display location, and rotation hints would yield different value (<code>0</code>, <code>90</code>, <code>180</code>, <code>270</code>) for each rotation.</p>
<p>The final answer is given by subtracting model's rotation hint from the image rotation given to the model. Implementing a voting scheme here gave surprisingly good results.</p>
<p>(image, voting)</p>
<p>Note that the hyper-parameter <code>K</code> doesn't need to be <code>4</code>. If the model is perfect, the maximum angle distance of the hinted rotation and the correct rotation is <code>(360 / K) / 2</code>. If <code>K</code> is bigger, the display detection problem is more difficult. On the other hand, the outputs are closer to being upright rotated, which makes the digit detection problem less difficult.</p>
<p>(image, comparison between two K values)</p>
<h4 id="digit-detection">Digit detection</h4>
<p>Digit detection inputs an upright display image and outputs rectangles labeled as digits: <code>digit_{0-9}</code>.</p>
<p>(image)</p>
<p>I expected this to be simple for the AutoML Object Detection. Lol, no. Nothing is simple. I got many precision losses which I couldn't easily explain.</p>
<p>(image)</p>
<p>Looking through the open sourced <code>object_detection</code> Tensorflow on GitHub, which I suspected AutoML was using, I found a parameter <a href="https://github.com/tensorflow/models/blob/master/official/vision/detection/dataloader/shapemask_parser.py#L131"><code>aug_rand_flip</code></a>. Based on the docs, the parameter expands input training images with their horizontal flips. This parameter can't be configured in AutoML Object Detection, but if they set it to true it would surely screw up 7-segment digit object detection.</p>
<p>(image)</p>
<p>Google Cloud support answered my question on <a href="https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/cloud-vision-discuss/6mrbUdWcVys/Tv4nXRy8BAAJ">cloud-vision-discuss thread</a>, which helped me understand that AI Platform Object Detection and AutoML Object Detection are quite different.</p>
<p>I submitted a training job to AI Platform (after building yet another pipeline to transform my data), and got excellent results. This was another surprise, because my training set was less than 400 images, and the AI Platform Object Detection trains from scratch - it doesn't do transfer learning.</p>
<h3 id="results">Results</h3>
<p>In the last iteration I used 468 images:</p>
<ul>
<li>317 images in training set</li>
<li>50 images in test set</li>
<li>101 images in validation set</li>
</ul>
<p>I never saw debug visualizations or debug log of the images in validation set - the evaluation pipeline would never produce them.</p>
<p>The final accuracies (the weight value correctly recognized):</p>
<ul>
<li>98.74% in training set</li>
<li>94.00% in test set</li>
<li>93.07% in validation set</li>
</ul>
<p>The detection latency is ~15 seconds, mostly because I optimized serving to be price efficient (free).</p>
<h2 id="conclusion">Conclusion</h2>
<h3 id="the-state-of-object-detection-support">The state of Object Detection support</h3>
<p>I am surprised by the simplicity and efficiency of Google Cloud AutoML Object Detection. When it comes to pricing, each account gets $300 free quota which is enough to train 5-10 model iterations. If you are lucky to get the result you need in that quota, everything else can be free - they give the ability to download the trained model as a Docker container, TF.js bundle or TF Lite bundle.</p>
<p>Using this free quota I managed to train a succcessful weight scale measurement OCR system.</p>
<h3 id="ml-principles-i-violated-and-got-burnt">ML principles I violated and got burnt</h3>
<p><strong>The distribution of data during prediction must align with the distirbution of data during training.</strong></p>
<p>I broke this principle multiple times. First, I thought that SVHN trained models would do a good job in recognizing scale weight measurement numbers. Lol, no. The transfer learning from SVHN model to my problem space also didn't end up working.</p>
<p>(image, difference between svhn and digits)</p>
<p>During display orinentation detection, I broke this principle again - I thought that the model trained on upright display images would give a low score to input images that are not upright. The score was not low - it was rubbish, sometimes high sometimes low.</p>
<p>When it comes to data collection phase, I think I got it right, at least from this principle's perspective. To collect the training dataset I built the application in its final form, with a fake model (myself as human labeler). Once the model was ready, I justed swapped out the human.</p>
<p><strong>Be evaluation driven.</strong></p>
<p>Early in the project when I was evaluating the related work, I would try to run the existing model and then feed it with an image or two to see how it works. Only when I started my own quality iterations I built a pipeline that runs the model on all train/test/val images and reports the metrics.</p>
<p>It would be better if I defined a container interface early on that listens to the image request on HTTP and gives a standardised JSON output. Then I would have a Docker image for every related work I managed to successfully run, persisted together with evaluation results. Having not done so, I only have weak (empirical) evidence that the related work models didn't work on my problem space.</p>
<p>When it comes to running things from other research-y works, the dependencies pose a big chalelnge. Docker solves this problem completely, and I am looking forward to seeing Docker adoption in academia.</p>
<p><strong>Be loss analysis driven.</strong></p>
<p>During loss analysis, it would happen to me that after I've seen 2 similar failures I would rush into implementing a hotfix for that specific pattern. This hotfix would always get reverted later on. Implementing hotfixes for specific losses (instead of biggest loss patterns) only increases the model complexity and, in my experience, it doesn't reflect to wins in validation set metric. The lesson learned here is to be patient, finish analyzing all losses that you prepared and then decide on the next step.</p>
<h3 id="parting-words">Parting words</h3>
<p>On Monday, 2020-10-28 I was in a room in downtown Manhattan packed with excited engineers and PMs who were ready to take over the world. This was AWS Data Lake Day in AWS Loft. I clearly remember a specific sentence - the speaker said confidently:</p>
<blockquote>
  <p>You've gotta use ML! If you don't, chances are that your competitors will.</p>
</blockquote>
<p>I hated it. On the other hand, in the audience I saw ripples of head-nodding folks getting high on agreement with the speaker.</p>
<p>I decided to embrace a different engineering principle - "Don't go AI first - always stay user first". ML is a tool, not a goal.</p>
<p>Even The Doors knew this when they played The Roadhouse Blues. They say:</p>
<blockquote>
  <p>Yeah, keep your eyes on the road, your hands upon the wheel.</p>
</blockquote>
<p>For <strong>the road</strong>, they probably meant <strong>user goal</strong> and for <strong>the wheel</strong> they meant <strong>the tools</strong> - whatever tools are needed - a for-loop, a compiler, a database or an ML model.</p>

        </div>
        <style type='text/css'>body {
  font: 400 16px/1.5 "Helvetica Neue", Helvetica, Arial, sans-serif;
  color: #111;
  background-color: #fdfdfd;
  -webkit-text-size-adjust: 100%;
  -webkit-font-feature-settings: "kern" 1;
  -moz-font-feature-settings: "kern" 1;
  -o-font-feature-settings: "kern" 1;
  font-feature-settings: "kern" 1;
  font-kerning: normal;
  padding: 30px;
}

@media only screen and (max-width: 600px) {
  body {
    padding: 5px;
  }

  body > #content {
    padding: 0px 20px 20px 20px !important;
  }
}

body > #content {
  margin: 0px;
  max-width: 900px;
  border: 1px solid #e1e4e8;
  padding: 10px 40px;
  padding-bottom: 20px;
  border-radius: 2px;
  margin-left: auto;
  margin-right: auto;
}

hr {
  color: #bbb;
  background-color: #bbb;
  height: 1px;
  flex: 0 1 auto;
  margin: 1em 0;
  padding: 0;
  border: none;
}

/**
 * Links
 */
a {
  color: #0366d6;
  text-decoration: none; }
  a:visited {
    color: #0366d6; }
  a:hover {
    color: #0366d6;
    text-decoration: underline; }

pre {
  background-color: #f6f8fa;
  border-radius: 3px;
  font-size: 85%;
  line-height: 1.45;
  overflow: auto;
  padding: 16px;
}

/**
  * Code blocks
  */

code {
  background-color: rgba(27,31,35,.05);
  border-radius: 3px;
  font-size: 85%;
  margin: 0;
  word-wrap: break-word;
  padding: .2em .4em;
  font-family: SFMono-Regular,Consolas,Liberation Mono,Menlo,Courier,monospace;
}

pre > code {
  background-color: transparent;
  border: 0;
  display: inline;
  line-height: inherit;
  margin: 0;
  overflow: visible;
  padding: 0;
  word-wrap: normal;
  font-size: 100%;
}


/**
 * Blockquotes
 */
blockquote {
  margin-left: 30px;
  margin-top: 0px;
  margin-bottom: 16px;
  border-left-width: 3px;
  padding: 0 1em;
  color: #828282;
  border-left: 4px solid #e8e8e8;
  padding-left: 15px;
  font-size: 18px;
  letter-spacing: -1px;
  font-style: italic;
}
blockquote * {
  font-style: normal !important;
  letter-spacing: 0;
  color: #6a737d !important;
}

/**
 * Tables
 */
table {
  border-spacing: 2px;
  display: block;
  font-size: 14px;
  overflow: auto;
  width: 100%;
  margin-bottom: 16px;
  border-spacing: 0;
  border-collapse: collapse;
}

td {
  padding: 6px 13px;
  border: 1px solid #dfe2e5;
}

th {
  font-weight: 600;
  padding: 6px 13px;
  border: 1px solid #dfe2e5;
}

tr {
  background-color: #fff;
  border-top: 1px solid #c6cbd1;
}

table tr:nth-child(2n) {
  background-color: #f6f8fa;
}

/**
 * Others
 */

img {
  max-width: 100%;
}

p {
  line-height: 24px;
  font-weight: 400;
  font-size: 16px;
  color: #24292e; }

ul {
  margin-top: 0; }

li {
  color: #24292e;
  font-size: 16px;
  font-weight: 400;
  line-height: 1.5; }

li + li {
  margin-top: 0.25em; }

* {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
  color: #24292e; }

a:visited {
  color: #0366d6; }

h1, h2, h3 {
  border-bottom: 1px solid #eaecef;
  color: #111;
  /* Darker */ }</style>
      </body>
    </html>